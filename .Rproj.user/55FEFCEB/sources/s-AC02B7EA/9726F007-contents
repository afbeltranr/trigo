---
title: "Dimension Reduction"

output:   

  prettydoc::html_pretty:
    theme: cayman
    

---
# Principal Components Analysis and Singular Value Decomposition

# Matrix data

Suppose we have some matrix data, and here we generate some random normal data. and we can see that the matrix plotted using the image function is not particularly interesting.

```{r}
set.seed(12345)
par(mar = rep(0.2, 4))
dataMatrix <- matrix (rnorm(400), nrow = 40)
image(1:10, 1:40,  t(dataMatrix)[, nrow(dataMatrix):1])
```

It looks pretty noisy and there is no real pattern. We can run a hierarchical clustering analysis on this data set, using the `heatmap()` function. In the output we can see that the clustering analysis si done, we get the dendrograms printed on both the columns and the rows, But again, there is no real interesting pattern that emerges, and that is because there is no real interesting pattern underlying in the data.

```{r}
par(mar = rep(0.2, 4))
heatmap(dataMatrix)
```


We can add a pattern to the data set. Let us ty to add something like this:

```{r}
set.seed(678910)
for(i in 1:40){
  # flip a coin
    coinFlip <- rbinom(1, size = 1, prob = 0.5)
    # if coin is heads (1) add a common pattern to that row
    
    if(coinFlip){
      dataMatrix[i,] <- dataMatrix[i,] + rep(c(0,3), each =5)
    }
  
 # if the coin is 1, then 5 the first 5 columns of that row have a mean of 0 and the other 5 columns have a mean of 3. 
  
}
```


```{r}
par(mar = rep(0.2,4))
image(1:10, 1:40 , t(dataMatrix)[, nrow(dataMatrix):1])
```
```{r}
par(mar = rep(0.2,4))
heatmap(dataMatrix)
```
If we run a hierarchical clustering analysis the dendrogram separates the two groups of columns.

# Patterns in rows and columns

```{r}
hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order,]
par(mfrow = c(1,3))
image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])
plot(rowMeans(dataMatrixOrdered), 
     40:1, ,
     xlab = 'Row mean', 
     ylab = 'Row', 
     pch = 19)
plot(colMeans(dataMatrixOrdered), 
     xlab = 'Column', 
     ylab = 'Column Mean', 
     pch = 19)
```
In the left hand plot we have the original matrix data, that has been reordered to look according to the hierarchical clustering analysis of the rows.

In the middle plot we plotted the mean of the rows. so if we look, on the y-axis we have the row number which goes from 1 to 40, so that is roughly parallel to the image on the left. And on the x-axis we got the mean of that row. For example, we can see that for row ten the mean is roughly -0.25, and for row 30 the mean is roughly 1.5.

```{r}
rev(rowMeans(dataMatrixOrdered))
```
We see taht there is a clear shift in the mean as you go across the rows. similarly, if you go across the columns, we can see that there is a clear shift in the mean of each column the first 5 of the 10 columns have a mean close to 0, and the second half of the columns have a mean close to 2.

# Related problems

You have multivariate variables $X_1 , ... , X_n$, so $X_1 = X_{11}, ... , X_{1m}$

* Find a new set of multivariate variables that are uncorrelated and explain as much variance as possible.
* Inf you put all the variables together in one matrix, find the best matrix created with fewer variables (lower rank) that explains the original data.   

The first goal is **statistical** (PCA) and the second goal is **data compression** (SVD)

# Related solutions - PCA/SVD

## SVD

If X is a matrix with each variable in a column and each observation in a row, then the SVD is a 'matrix decomposition'


\begin{align}

X = UDV^T
\end{align}


Where the columns of U are orthogonal (left singular vectors), the columns of V are orthogonal (right singular vectors) and D is a singular matrix (singular values)

## PCA

The principal components are equal to the right singular values if you first scale (subtract the mean, divide by the standard deviation) the variables.

# Components of the SVD - *u* and *v*

If we break down to the components of the singular value decomposition we can look at the **u** and the **v** as the left singular values and the right singular values.

```{r}
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1,3))
image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])

plot(svd1$u[,1], 
     40:1,
     xlab = 'First left singular vector',
     ylab = 'Row',
     pch = 19)
plot(svd1$v[,1], 
     xlab = 'Column',
     ylab = 'First Right singualr vector',
     pch = 19)

```

Here, plotting the first left singular vector it roughly shows the pattern of information defined by the mean of the rows of the data set. so if you plot it across the rows, you can see that the first left singular vector has a negative value for rows 40 through about 18 or 17. And then it has a positive value for the remaining rows. So that shows the clear separation in the means of the two sets of rows. 

If we look af the first right singular vector we can see that it also shows the change in the mean between the first five columns ad the second five columns. And so the nice thing about the singular value decomposition here is that immediately picked up on the shift in the means, both in the row dimension and the column dimension.

This happened without us asking for something. this find was totally unsupervised.

Another component of the singular value decomposition is known as the variance explained, And this comes from the singular values that are in the D matrix, the diagonal matrix. It only has elements that are in the diagonal of that matrix. And you can think of each singular value as representing the percent of the total variation in your data. The components are typically ordered so the first one explains the most variation as possible, and the second one explains the second most, etc. You can plot the proportion of variance explained as we have in the plot below :


```{r}
par(mfrow = c(1,2))
plot(svd1$d,
     xlab = 'column', 
     ylab = 'Singular value',
     pch = 19)

plot(svd1$d^2/sum(svd1$d^2),
     xlab = 'column', 
     ylab = 'Prop. of variance explained',
     pch = 19)
```


in the left hand side we have the raw singular value and in the right hand side we have the proportion of variance explained by each dimension. The first singular value, if we remember captures the information about the shift in the mean between rows and columns. That singular values captures 40% of the variation in the data. The remaining variation is explained by the other components.


Just to show thet the relationship between the singular value decomposition and the principal components is close, here we will calculate the SVD and the PCA for the same data matrix :

```{r}
svd1 <- svd(scale(dataMatrixOrdered))
pca1 <- prcomp(dataMatrixOrdered, scale = TRUE)
plot(pca1$rotation[,1], 
     svd1$v[,1],
     pch = 19,
     xlab = 'Principal Component 1',
     ylab = 'Right singular vector 1')
abline(c(0,1))
```

# Components of the SVD - variance explained

Here we have a very basic example which is just a matrix that has either zeros or ones, And the idea is that there is only one pattern in this matrix. The first five columns are zeros and the secondfive columns are ones.


```{r}
constantMatrix <- dataMatrixOrdered*0
for(i in 1:dim(dataMatrixOrdered)[1]){
  
  constantMatrix[i,] <- rep(c(0,1), each = 5)
}
svd1 <- svd(constantMatrix)
par(mfrow = c(1,3))
image(t(constantMatrix)[, nrow(constantMatrix):1])
plot(svd1$d,
     xlab = 'Column',
     ylab = 'Singular value',
     pch = 19)
plot(svd1$d^2/sum(svd1$d^2),
     xlab = 'Column',
     ylab = 'Singular value',
     pch = 19)

```


# What if we add a second pattern? 



Let us add a second pattern to the data set, So we can add a pattern that goes across the rows and also across the columns, The first pattern will be defined by the coin flipping, selecting in which rows the change of the mean will be performed, and the second pattern will be the change of means in the columns.


```{r}
set.seed(678910)
for(i in 1:40) {
  
  # flip a coin
  coinFlip1 <- rbinom(1, size = 1 , prob = 0.5)
  coinFlip2 <- rbinom(1, size = 1 , prob = 0.5)
  # if coin is heads (1) add a common pattern to that row
  if(coinFlip1){
    dataMatrix[i,] <- dataMatrix[i,] + rep(c(0,5), each = 5) }# Here the sequence is c(0,0,0,0,0,5,5,5,5,5)
    if(coinFlip2){
    dataMatrix[i,] <- dataMatrix[i,] + rep(c(0,5), 5) #here the sequence c(0,5) is repeated 5 times c(0,5,0,5,0,5,0,5,0,5)
  }
  
  }  
  
hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order,]
```

```{r}
svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1,3))
image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])

plot(
  rep(c(0,1), each = 5),
  pch = 19, 
  xlab = 'Column',
  ylab = 'Pattern 1'
  
)

plot(
  rep(c(0,1), 5),
  pch = 19, 
  xlab = 'Column',
  ylab = 'Pattern 2'
  
)
```

Above we see the truth about the patterns we generated in the data matrix, one is a shift pattern and the other is an alternating pattern.

```{r}
svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1,3))
image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])

plot(svd2$v[,1],
  pch = 19, 
  xlab = 'Column',
  ylab = 'First right singular vector'
  
)

plot(svd2$v[,2],
  pch = 19, 
  xlab = 'Column',
  ylab = 'Second right singular vector'
  
)
```

In the middle panel here, we have got the first right singular vector. it roughly picks up the block patter, The first five are somewhat lower, and the second five are somewhat higher. It is not as pretty as the truth was, but is somewhat discernible, that there are two different kind of means here.

The second right singular vector is on the right-hand panel here.  And you can see that it is a little bit harder to see, but it does try to pick up the alternating mean pattern. It is not as obvious as it was when we were plotting the truth, of course. But you can see that every other point is either higher or lower. Now of course since we know the truth it is easier to talk about the patterns.


In general we can see that the two patterns are roughly confounded with each other because it is a little bit hart do set the two patterns apart. The first and second right singular vectors are also known as teh principal componets.

They mix the two patterns together, so each of them has a block pattern ad ech of them has somewhat an alternating pattern. And so unfortunately, just like with most real data, the truth is a little bit harder to discern than if you had known it in advance.

# d and variance explained.

```{r}
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1,2))
plot(svd1$d,
     xlab = 'Column',
     ylab = 'Singular value',
     pch = 19)
plot(svd1$d^2/sum(svd1$d^2)*100,
     xlab = 'Column',
     ylab = 'Percent variance explained',
     pch = 19)

```


Now if we look at the variance explained, the first component explains over 50% of the that variation in the data set.

If we remember, the first singular value or component, represents the shift pattern over the columns. This shift pattern represent a large variation in the data set, so if we capture that shift pattern, we pature a lot of that variation.

The second component only captures about 18% of the variation, and it trails off from there. The ateltarting pattern of every otrher column is a little bit harder to puck up.


# Missing values

One issue with either SVD or PCA is missing values. So real data will typically have missing values, and the problem is that if you try to run the SVD on a data set that does not have or that has some missing values, like we will create, you just cant run it to that data set, so we need to do something about the missin values before run an SVD o PCA.

```{r eval = FALSE}
dataMatrix2 <- dataMatrixOrdered
## RAndomly insert some missing data

dataMatrix2[sample(1:100, size =40, replace = FALSE)] <- NA
svd1 <- svd(scale(dataMatrix2)) ## Does not work !!

# Error in svd(scale(dataMatrix2)) : infinite or missing values in 'x'
```

One possibility and there are many others, is use the ``impute` package which is available from the bioconductor project.
to install this package:
```{r eval = FALSE}
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("impute")
```


```{r}
library(impute)
dataMatrix2 <- dataMatrixOrdered
## RAndomly insert some missing data

dataMatrix2[sample(1:100, size =40, replace = FALSE)] <- NA
dataMatrix2 <- impute.knn(dataMatrix2)$data
svd1 <- svd(scale(dataMatrixOrdered))
svd2 <- svd(scale(dataMatrix2))
par(mfrow=c(1,2))
plot(svd1$v[,1],
     pch = 19)
plot(svd2$v[,1],
     pch= 19)
```

I imputes the missing values so we can run the SVD. this code uses the `impute.knn` function which takes a misson row or missing values in a row, and imputes it by the k-nearest neighbors to that row. So if k, for example is five, then it will take the five rows that are closest to the row with the missing data, and then impute the data in that missing row with the average of the other five. 

Once we have imputed the data, we can run the SVD, and we can see they are roughly similar, the imputation did not have a significant effect.

Let us see a final example. We can take an actual image, which is represented as a matrix, and develop a lower dimensional or lower rank representation of this actual image.
```{r}
load('face.rda')
image(t(faceData)[, nrow(faceData):1])
```

So here we have an image of a face, it is at a relatively low reolution picture of a face, but we can see that there is a nose, and ears and two eyes and a mouth. And so what we are going to do is run the svd on this face data and look at the variance explained.

```{r}
svd1 <- svd(scale(faceData))
plot(svd1$d^2/sum(svd1$d^2),
     pch = 19,
     xlab= 'Singular vector',
     ylab = 'variance explained')
```

The first five to ten singular vectors capture about all the information in the image.


# Create approximations

We can use a little bit of matrix multiplication to create an approximation of the face, using fewer components that the original data set.


```{r}
svd1 <- svd(scale(faceData))
## Note that %*% is matrix multiplication

# Here svd1$d[1] is a constant
approx1 <- svd1$u[,1] %*% t(svd1$v[,1]) * svd1$d[1]


# in these examples we need to make the diagonal matrix out of d

approx5 <- svd1$u[, 1:5] %*% diag(svd1$d[1:5]) %*% t(svd1$v[,1:5])
approx10 <- svd1$u[, 1:10] %*% diag(svd1$d[1:10]) %*% t(svd1$v[,1:10])


```

```{r}
par(mfrow = c(1,4), mar = rep(0.2,4))
image(t(approx1)[,nrow(approx1):1], main = '(a)')
image(t(approx5)[,nrow(approx5):1], main = '(b)')
image(t(approx10)[,nrow(approx10):1], main = '(c)')
image(t(faceData)[,nrow(faceData):1], main = '(d)') ## Original Data
```

# Notes

* Scale matters
* PCs / SVs may mex real patterns
* Can be computationally intensive
* Alternatives
 + Factor analysis
 + Independent Components analysis
 
